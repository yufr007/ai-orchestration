"""Tester agent: Test generation and execution."""

import asyncio
import subprocess
from datetime import datetime
from typing import Any

from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

from src.config import get_settings
from src.core.state import AgentResult, AgentRole, OrchestrationState, TaskStatus
from src.tools.github import get_file_contents, add_pr_comment

settings = get_settings()

TESTER_SYSTEM_PROMPT = """You are an elite QA/Test Engineer ensuring code quality and correctness.

Your responsibilities:
1. Analyze implemented code to understand functionality
2. Generate comprehensive test cases (unit, integration, edge cases)
3. Identify potential bugs and issues
4. Verify error handling and edge cases
5. Check for security vulnerabilities
6. Ensure tests are maintainable and follow best practices

For each file changed, generate:
- Unit tests covering core functionality
- Edge case tests (null, empty, invalid inputs)
- Integration tests if dependencies exist
- Performance tests for critical paths

Output test files in the project's testing framework (pytest, jest, etc.)."""


async def generate_tests_for_file(llm: ChatAnthropic, repo: str, file_path: str, branch: str) -> str:
    """Generate tests for a specific file."""
    print(f"  ğŸ§ª Generating tests for {file_path}...")

    try:
        content = await get_file_contents(repo, file_path, branch)
    except Exception as e:
        print(f"    âœ— Failed to read {file_path}: {e}")
        return ""

    messages = [
        SystemMessage(content=TESTER_SYSTEM_PROMPT),
        HumanMessage(
            content=f"""Generate comprehensive tests for this code:

**File:** {file_path}
```
{content}
```

Provide complete test file contents using pytest."""
        ),
    ]

    response = await llm.ainvoke(messages)
    return response.content


async def tester_node(state: OrchestrationState) -> dict[str, Any]:
    """Execute the tester agent to generate and run tests."""
    print("\nğŸ§ª TESTER: Starting testing phase...")

    files_changed = state.get("files_changed", [])
    repo = state["repo"]
    branch = state.get("branches_created", [None])[-1]
    pr_number = state.get("prs_created", [None])[-1]

    if not files_changed:
        return {
            "test_results": {"passed": True, "message": "No files to test"},
            "agent_results": [
                AgentResult(
                    agent=AgentRole.TESTER,
                    status=TaskStatus.SKIPPED,
                    output="No files changed",
                    artifacts={},
                    metadata={},
                    timestamp=datetime.now(),
                )
            ],
        }

    # Initialize LLM
    llm = ChatAnthropic(
        model=settings.default_agent_model,
        temperature=0.2,
        api_key=settings.anthropic_api_key,
    )

    # Generate tests for changed files
    print(f"ğŸ“ TESTER: Generating tests for {len(files_changed)} files...")
    test_generations = await asyncio.gather(
        *[generate_tests_for_file(llm, repo, file, branch) for file in files_changed],
        return_exceptions=True,
    )

    # Analyze test coverage
    test_failures = []
    total_tests_generated = 0

    for file_path, test_content in zip(files_changed, test_generations):
        if isinstance(test_content, Exception):
            test_failures.append({"file": file_path, "error": str(test_content)})
        elif test_content:
            total_tests_generated += 1

    # For now, we'll mark as passed if tests were generated
    # In production, you'd actually run the tests
    test_results = {
        "passed": len(test_failures) == 0,
        "total_files": len(files_changed),
        "tests_generated": total_tests_generated,
        "failures": test_failures,
    }

    # Add comment to PR
    if pr_number:
        comment = f"""## ğŸ§ª Test Analysis

**Files tested:** {len(files_changed)}
**Tests generated:** {total_tests_generated}
**Status:** {'âœ… PASS' if test_results['passed'] else 'âŒ FAIL'}

{'### âš ï¸ Failures\n' + '\n'.join(f"- `{f['file']}`: {f['error']}" for f in test_failures) if test_failures else '### âœ… All tests generated successfully'}

---
*Generated by AI Orchestration Platform - Tester Agent*
"""
        await add_pr_comment(repo, pr_number, comment)

    print(
        f"{'âœ…' if test_results['passed'] else 'âŒ'} TESTER: Testing complete - {total_tests_generated} tests generated"
    )

    return {
        "test_results": test_results,
        "test_failures": test_failures,
        "current_agent": AgentRole.TESTER,
        "agent_results": [
            AgentResult(
                agent=AgentRole.TESTER,
                status=TaskStatus.COMPLETED if test_results["passed"] else TaskStatus.FAILED,
                output=f"Generated {total_tests_generated} test files",
                artifacts={"test_results": test_results},
                metadata={"failures": len(test_failures)},
                timestamp=datetime.now(),
            )
        ],
    }
